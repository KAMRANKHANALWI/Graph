"""
Web Crawler using Graphs
=========================================
Learn how web crawlers work using graph algorithms (BFS/DFS).
"""

from collections import deque


class SimpleWebCrawler:
    """
    A simple web crawler that treats websites as a graph
    """

    def __init__(self):
        self.website_links = {}  # website -> list of linked websites
        self.visited_sites = set()
        self.crawl_results = []

    def add_website(self, site, links):
        """
        Add a website and its outgoing links
        """
        self.website_links[site] = links
        print(f"ğŸ“„ Added website: {site} with {len(links)} links")

    def crawl_bfs(self, start_site, max_pages=10):
        """
        Crawl websites using BFS (breadth-first)
        Explores level by level like real crawlers
        """
        print(f"ğŸŒ Starting BFS crawl from: {start_site}")
        print("=" * 40)

        if start_site not in self.website_links:
            print(f"âŒ Starting site {start_site} not found!")
            return []

        visited = set()
        queue = deque([start_site])
        visited.add(start_site)
        crawled_sites = []

        while queue and len(crawled_sites) < max_pages:
            current_site = queue.popleft()
            crawled_sites.append(current_site)

            print(f"  ğŸ” Crawling: {current_site}")

            # Get all links from current site
            links = self.website_links.get(current_site, [])
            print(f"    Found {len(links)} links: {links}")

            # Add unvisited links to queue
            for link in links:
                if link not in visited and link in self.website_links:
                    visited.add(link)
                    queue.append(link)
                    print(f"      â• Added to queue: {link}")

        print(f"\nâœ… BFS Crawl complete! Visited {len(crawled_sites)} pages")
        return crawled_sites

    def crawl_dfs(self, start_site, max_pages=10):
        """
        Crawl websites using DFS (depth-first)
        Goes deep into one path before backtracking
        """
        print(f"ğŸ•³ï¸ Starting DFS crawl from: {start_site}")
        print("=" * 40)

        if start_site not in self.website_links:
            print(f"âŒ Starting site {start_site} not found!")
            return []

        visited = set()
        crawled_sites = []

        def dfs_helper(site):
            if len(crawled_sites) >= max_pages:
                return

            visited.add(site)
            crawled_sites.append(site)
            print(f"  ğŸ” Crawling: {site}")

            # Get all links from current site
            links = self.website_links.get(site, [])
            print(f"    Found {len(links)} links: {links}")

            # Visit each unvisited link
            for link in links:
                if link not in visited and link in self.website_links:
                    print(f"      â¡ï¸ Going deeper to: {link}")
                    dfs_helper(link)

        dfs_helper(start_site)
        print(f"\nâœ… DFS Crawl complete! Visited {len(crawled_sites)} pages")
        return crawled_sites

    def find_path_between_sites(self, start_site, target_site):
        """
        Find if you can get from one website to another by following links
        """
        print(f"ğŸ¯ Finding path: {start_site} â†’ {target_site}")

        if start_site not in self.website_links:
            return None

        visited = set([start_site])
        queue = deque([(start_site, [start_site])])

        while queue:
            current_site, path = queue.popleft()

            if current_site == target_site:
                print(f"  âœ… Path found: {' â†’ '.join(path)}")
                return path

            for link in self.website_links.get(current_site, []):
                if link not in visited and link in self.website_links:
                    visited.add(link)
                    queue.append((link, path + [link]))

        print(f"  âŒ No path from {start_site} to {target_site}")
        return None

    def analyze_website_network(self):
        """
        Analyze the website network structure
        """
        print("ğŸ“Š WEBSITE NETWORK ANALYSIS")
        print("=" * 30)

        total_sites = len(self.website_links)
        total_links = sum(len(links) for links in self.website_links.values())

        print(f"ğŸŒ Total websites: {total_sites}")
        print(f"ğŸ”— Total links: {total_links}")

        if total_sites > 0:
            avg_links = total_links / total_sites
            print(f"ğŸ“Š Average links per site: {avg_links:.1f}")

        # Find most connected sites
        site_connections = [
            (site, len(links)) for site, links in self.website_links.items()
        ]
        site_connections.sort(key=lambda x: x[1], reverse=True)

        print(f"\nğŸŒŸ Most connected sites:")
        for i, (site, links) in enumerate(site_connections[:3], 1):
            print(f"   {i}. {site} ({links} outgoing links)")

        # Find sites with no outgoing links (dead ends)
        dead_ends = [site for site, links in self.website_links.items() if not links]
        if dead_ends:
            print(f"\nğŸ”š Dead end sites (no outgoing links): {dead_ends}")


def create_example_website_network():
    """
    Create a simple website network for testing
    """
    print("ğŸ—ï¸ CREATING EXAMPLE WEBSITE NETWORK")
    print("=" * 38)

    crawler = SimpleWebCrawler()

    # Add websites and their links (simplified)
    websites = {
        "google.com": ["youtube.com", "gmail.com", "maps.google.com"],
        "youtube.com": ["google.com", "music.youtube.com"],
        "gmail.com": ["google.com", "calendar.google.com"],
        "facebook.com": ["instagram.com", "whatsapp.com"],
        "instagram.com": ["facebook.com"],
        "whatsapp.com": ["facebook.com"],
        "twitter.com": ["tweetdeck.com"],
        "tweetdeck.com": ["twitter.com"],
        "maps.google.com": ["google.com"],
        "music.youtube.com": ["youtube.com"],
        "calendar.google.com": ["gmail.com"],
        "reddit.com": ["news.reddit.com"],
        "news.reddit.com": ["reddit.com"],
    }

    # Add all websites to crawler
    for site, links in websites.items():
        crawler.add_website(site, links)

    return crawler


def demonstrate_web_crawling():
    """
    Show how web crawling works with both BFS and DFS
    """
    crawler = create_example_website_network()

    print("\n" + "=" * 50)

    # Analyze the network first
    crawler.analyze_website_network()

    print("\n" + "=" * 50)

    # Compare BFS vs DFS crawling
    start_site = "google.com"
    max_pages = 8

    print("ğŸ†š COMPARING CRAWL STRATEGIES")
    print("=" * 30)

    # BFS crawl
    print(f"\n1ï¸âƒ£ BFS Crawl (explores level by level):")
    bfs_results = crawler.crawl_bfs(start_site, max_pages)
    print(f"   BFS visited: {bfs_results}")

    print(f"\n2ï¸âƒ£ DFS Crawl (goes deep first):")
    dfs_results = crawler.crawl_dfs(start_site, max_pages)
    print(f"   DFS visited: {dfs_results}")

    print(f"\nğŸ“Š Comparison:")
    print(f"   BFS explores broadly first")
    print(f"   DFS explores one path deeply first")
    print(f"   Both visit the same sites, just in different order!")


def show_link_analysis():
    """
    Show how to analyze connections between websites
    """
    print("\nğŸ”— LINK ANALYSIS EXAMPLES")
    print("=" * 30)

    crawler = create_example_website_network()

    # Test different paths
    test_paths = [
        ("google.com", "music.youtube.com"),
        ("facebook.com", "instagram.com"),
        ("google.com", "reddit.com"),
        ("twitter.com", "facebook.com"),
    ]

    for start, end in test_paths:
        crawler.find_path_between_sites(start, end)


def real_world_applications():
    """
    Explain real-world applications of web crawling
    """
    print("\nğŸŒ REAL-WORLD WEB CRAWLING")
    print("=" * 30)

    print(
        """
ğŸš€ HOW WEB CRAWLERS ARE USED:

1. ğŸ” SEARCH ENGINES
   â€¢ Google, Bing crawl billions of pages
   â€¢ Index content for search results  
   â€¢ Follow links to discover new pages
   â€¢ Use BFS to explore systematically

2. ğŸ“Š DATA COLLECTION
   â€¢ Price comparison websites
   â€¢ Social media analysis
   â€¢ News aggregation
   â€¢ Market research

3. ğŸ›¡ï¸ SECURITY SCANNING
   â€¢ Find vulnerabilities in websites
   â€¢ Check for broken links
   â€¢ Monitor website changes
   â€¢ Detect malicious content

4. ğŸ“ˆ SEO ANALYSIS
   â€¢ Analyze website structure
   â€¢ Find internal linking issues
   â€¢ Check page accessibility
   â€¢ Monitor competitor sites

5. ğŸ“š ARCHIVING
   â€¢ Wayback Machine saves web history
   â€¢ Digital preservation
   â€¢ Academic research
   â€¢ Legal compliance

ğŸ¤– CRAWLER STRATEGIES:

â€¢ BFS (Breadth-First): 
  âœ… Good for finding all pages
  âœ… Discovers important pages first
  âŒ Uses more memory

â€¢ DFS (Depth-First):
  âœ… Uses less memory
  âœ… Good for deep site exploration
  âŒ Might miss important pages

â€¢ Focused Crawling:
  âœ… Targets specific topics
  âœ… More efficient
  âœ… Better quality results
"""
    )


def simple_crawler_rules():
    """
    Basic rules and ethics of web crawling
    """
    print("\nğŸ“‹ WEB CRAWLING BEST PRACTICES")
    print("=" * 35)

    print(
        """
âœ… GOOD PRACTICES:

1. ğŸ¤– RESPECT ROBOTS.TXT
   â€¢ Check /robots.txt file
   â€¢ Follow crawling rules
   â€¢ Respect disallowed areas

2. â±ï¸ BE POLITE WITH REQUESTS
   â€¢ Add delays between requests
   â€¢ Don't overload servers
   â€¢ Respect rate limits

3. ğŸ†” IDENTIFY YOURSELF
   â€¢ Use proper User-Agent
   â€¢ Provide contact information
   â€¢ Be transparent about purpose

4. ğŸ“ FOLLOW TERMS OF SERVICE
   â€¢ Read website policies
   â€¢ Don't violate copyright
   â€¢ Respect data usage rules

âŒ THINGS TO AVOID:

â€¢ Don't crawl too fast
â€¢ Don't ignore robots.txt
â€¢ Don't scrape personal data
â€¢ Don't overload small sites
â€¢ Don't violate copyright

ğŸ›¡ï¸ TECHNICAL CONSIDERATIONS:

â€¢ Handle JavaScript-rendered pages
â€¢ Manage cookies and sessions
â€¢ Deal with CAPTCHAs
â€¢ Handle redirects properly
â€¢ Implement error handling
"""
    )


if __name__ == "__main__":
    # Main demonstration
    demonstrate_web_crawling()

    # Link analysis
    show_link_analysis()

    # Real-world applications
    real_world_applications()

    # Best practices
    simple_crawler_rules()

    print("\nâœ… You now understand web crawling with graphs!")
    print("ğŸ¯ Key insight: Web crawling is just graph traversal on the internet!")
